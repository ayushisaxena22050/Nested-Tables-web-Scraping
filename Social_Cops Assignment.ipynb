{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.support import expected_conditions as EC \n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import requests\n",
    "import lxml.html as lh\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping State wise Data\n",
    "url=\"http://swachhbharaturban.gov.in/ihhl/RPTApplicationSummary.aspx\"\n",
    "page = requests.get(url)\n",
    "page_content = BeautifulSoup(page.content, \"html.parser\")\n",
    "table=page_content.find('table', id='ContentPlaceHolder1_gvApplicationListState')\n",
    "rows=table.findAll('tr')\n",
    "rows[1].append(rows[2])\n",
    "headers=rows[1].text.split('\\n')\n",
    "headers=[x.strip() for x in headers]\n",
    "headers=[x for x in headers if x]\n",
    "headers=['SNO',\n",
    " 'State',\n",
    " 'No. of Applications Received',\n",
    " 'No. of Applications Not Verified',\n",
    " 'No. of Applications Verified',\n",
    " 'No. of Applications Approved',\n",
    " 'No. of Applications Approved having Aadhar No.',\n",
    " 'No. of Applications Rejected',\n",
    " 'No. of Applications Pullback',\n",
    " 'No. of Applications Closed',\n",
    " 'No. of Constructed Toilet Photo Uploaded',\n",
    " 'No. of Constructed Toilet Photo Approved',\n",
    " 'No. of Commenced Toilet Photo ',\n",
    " 'No. of Constructed Toilet Photo through Swachhalaya',\n",
    " ]\n",
    "# starting 1,2 rows are for headers and 0 row was blank\n",
    "full_data=[]\n",
    "for i in range(3,len(rows)):\n",
    "    full_data.append(rows[i].text.split('\\n'))\n",
    "clean_data=[]\n",
    "for i in range(len(full_data)):\n",
    "    space_to_empty=[x.strip() for x in full_data[i]]\n",
    "    space_clean_list=[x for x in space_to_empty if x]\n",
    "    clean_data.append(space_clean_list)\n",
    "final_table=pd.DataFrame(clean_data)\n",
    "final_table=final_table.drop_duplicates()\n",
    "#column 13 just have symbol, so I am going to drop it\n",
    "del final_table[13]\n",
    "final_table.columns=headers\n",
    "final_table.to_csv('State_wise_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping state_ids and state_names to fetch district data.\n",
    "timeout = 20\n",
    "browser = webdriver.Firefox()\n",
    "browser.get('http://swachhbharaturban.gov.in/ihhl/RPTApplicationSummary.aspx')\n",
    "browser.set_page_load_timeout(timeout)\n",
    "html=browser.page_source\n",
    "soup=BeautifulSoup(html)\n",
    "links_for_states=soup.findAll('a')\n",
    "state_ids=[]\n",
    "for i in range(2,len(links_for_states)-1):\n",
    "    state_ids.append(links_for_states[i]['id'])\n",
    "state_name=[]\n",
    "for i in range(2,len(links_for_states)-1):\n",
    "    state_name.append(links_for_states[i].text)\n",
    "headers=['SNO',\n",
    "     'District',\n",
    "     'No. of Applications Received',\n",
    "     'No. of Applications Not Verified',\n",
    "     'No. of Applications Verified',\n",
    "     'No. of Applications Approved',\n",
    "     'No. of Applications Approved having Aadhar No.',\n",
    "     'No. of Applications Rejected',\n",
    "     'No. of Applications Pullback',\n",
    "     'No. of Applications Closed',\n",
    "     'No. of Constructed Toilet Photo Uploaded',\n",
    "     'No. of Constructed Toilet Photo Approved',\n",
    "     'No. of Commenced Toilet Photo ',\n",
    "     'No. of Constructed Toilet Photo through Swachhalaya',\n",
    "     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping District Wise Data\n",
    "timeout = 200\n",
    "browser = webdriver.Firefox()\n",
    "browser.get('http://swachhbharaturban.gov.in/ihhl/RPTApplicationSummary.aspx')\n",
    "browser.set_page_load_timeout(timeout)\n",
    "a=pd.DataFrame()\n",
    "for j in range(0,len(state_ids)):\n",
    "    browser.find_element_by_id(state_ids[j]).click()\n",
    "    element= WebDriverWait(browser, 10).until(lambda x: x.find_element_by_id('ContentPlaceHolder1_gvApplicationListDistrict'))\n",
    "    soup2=BeautifulSoup(browser.page_source)\n",
    "    soup=soup2.find('table',id='ContentPlaceHolder1_gvApplicationListDistrict')\n",
    "    element2= WebDriverWait(browser, 10).until(lambda x: x.find_element_by_id('ContentPlaceHolder1_gvApplicationListDistrict'))\n",
    "    rows2=soup.findAll('tr')\n",
    "    full_data_district=[]\n",
    "    for i in range(4,len(rows2)):\n",
    "        full_data_district.append(rows2[i].text.split('\\n'))\n",
    "    clean_data_district=[]\n",
    "    for k in range(len(full_data_district)):\n",
    "        space_to_empty=[x.strip() for x in full_data_district[k]]\n",
    "        space_clean_list=[x for x in space_to_empty if x]\n",
    "        clean_data_district.append(space_clean_list)\n",
    "    final_table_district=pd.DataFrame(clean_data_district)\n",
    "    final_table_district=final_table_district.drop_duplicates()\n",
    "    \n",
    "    #column 13 just have symbol, so I am going to drop it\n",
    "    del final_table_district[13]\n",
    "    final_table_district.columns=headers\n",
    "    final_table_district.insert(1,'State',state_name[j])\n",
    "    a=pd.concat([a,final_table_district])\n",
    "    browser.get('http://swachhbharaturban.gov.in/ihhl/RPTApplicationSummary.aspx')\n",
    "\n",
    "browser.quit()\n",
    "    \n",
    "district_wise_data=a\n",
    "district_wise_data=district_wise_data.dropna()\n",
    "district_wise_data.to_csv('District_wise_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping ULB Wise Data\n",
    "timeout = 200\n",
    "browser = webdriver.Firefox()\n",
    "browser.get('http://swachhbharaturban.gov.in/ihhl/RPTApplicationSummary.aspx')\n",
    "browser.set_page_load_timeout(timeout)\n",
    "html=browser.page_source\n",
    "state_html=BeautifulSoup(html)\n",
    "state_tags=state_html.findAll('a')\n",
    "headers=['SNO',\n",
    "             'ulb',\n",
    "             'No. of Applications Received',\n",
    "             'No. of Applications Not Verified',\n",
    "             'No. of Applications Verified',\n",
    "             'No. of Applications Approved',\n",
    "             'No. of Applications Approved having Aadhar No.',\n",
    "             'No. of Applications Rejected',\n",
    "             'No. of Applications Pullback',\n",
    "             'No. of Applications Closed',\n",
    "             'No. of Constructed Toilet Photo Uploaded',\n",
    "             'No. of Constructed Toilet Photo Approved',\n",
    "             'No. of Commenced Toilet Photo ',\n",
    "             'No. of Constructed Toilet Photo through Swachhalaya',\n",
    "             ]\n",
    "a=pd.DataFrame()\n",
    "for j in range(2,len(state_tags)-1):\n",
    "    WebDriverWait(browser, 10).until(lambda x: x.find_element_by_id('ContentPlaceHolder1_gvApplicationListState'))\n",
    "    state_click=browser.find_element_by_id(state_tags[j]['id'])\n",
    "    state_click.click()\n",
    "    state=state_tags[j].text\n",
    "    element= WebDriverWait(browser, 10).until(lambda x: x.find_element_by_id('ContentPlaceHolder1_gvApplicationListDistrict'))\n",
    "    soup2=BeautifulSoup(browser.page_source)\n",
    "    soup=soup2.find('table',id='ContentPlaceHolder1_gvApplicationListDistrict')\n",
    "    element2= WebDriverWait(browser, 10).until(lambda x: x.find_element_by_id('ContentPlaceHolder1_gvApplicationListDistrict'))\n",
    "    districts=soup.findAll('a')\n",
    "    for i in range(0,len(districts)):\n",
    "        WebDriverWait(browser, 10).until(lambda x: x.find_element_by_id('ContentPlaceHolder1_gvApplicationListDistrict'))\n",
    "        element=browser.find_element_by_id(districts[i]['id'])\n",
    "        element.click()\n",
    "        district=districts[i].text\n",
    "        element= WebDriverWait(browser, 10).until(lambda x: x.find_element_by_id('ContentPlaceHolder1_gvApplicationListULB'))\n",
    "        soup3=BeautifulSoup(browser.page_source)\n",
    "        soup4=soup3.find('table',id='ContentPlaceHolder1_gvApplicationListULB')\n",
    "        ulb=soup4.findAll('tr')\n",
    "        if len(ulb)<4:\n",
    "            b=[0]*14\n",
    "            c=pd.DataFrame(b)\n",
    "            c=c.transpose()\n",
    "            final_table_ulb=c  \n",
    "        full_data_ulb=[]\n",
    "        for k in range(4,len(ulb)): \n",
    "            full_data_ulb.append(ulb[k].text.split('\\n'))\n",
    "        clean_data_ulb=[]\n",
    "        for k in range(len(full_data_ulb)):\n",
    "            space_to_empty=[x.strip() for x in full_data_ulb[k]]\n",
    "            space_clean_list=[x for x in space_to_empty if x]\n",
    "            clean_data_ulb.append(space_clean_list)\n",
    "            final_table_ulb=pd.DataFrame(clean_data_ulb)\n",
    "            final_table_ulb=final_table_ulb.drop_duplicates()\n",
    "            #column 13 just have symbol, so I am going to drop it\n",
    "            del final_table_ulb[13]\n",
    "        \n",
    "        final_table_ulb.columns=headers\n",
    "        final_table_ulb.insert(1,'State',state)\n",
    "        final_table_ulb.insert(2,'District',district)\n",
    "        a=pd.concat([a,final_table_ulb])\n",
    "        browser.find_element_by_link_text(state_tags[j].text.strip()).click()\n",
    "    browser.get('http://swachhbharaturban.gov.in/ihhl/RPTApplicationSummary.aspx')\n",
    "browser.quit()\n",
    "ulb_wise_data=a.dropna()\n",
    "ulb_wise_data.to_csv('ulb_wise_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping Ward_wise_data\n",
    "# In state tags, first two rows are noisy data ,so we are skipping those and counter will start from 2\n",
    "# I have parsed it in six chunks,from states 1-6,6-12,12-18,18-24,24-30,30-36 as it is too much data to be parsed at once.\n",
    "timeout = 200\n",
    "browser = webdriver.Firefox()\n",
    "browser.get('http://swachhbharaturban.gov.in/ihhl/RPTApplicationSummary.aspx')\n",
    "browser.set_page_load_timeout(timeout)\n",
    "browser.implicitly_wait(30)\n",
    "html=browser.page_source\n",
    "state_html=BeautifulSoup(html)\n",
    "state_tags=state_html.findAll('a')\n",
    "headers=['SNO',\n",
    "             'Ward No.',\n",
    "             'No. of Applications Received',\n",
    "             'No. of Applications Not Verified',\n",
    "             'No. of Applications Verified',\n",
    "             'No. of Applications Approved',\n",
    "             'No. of Applications Approved having Aadhar No.',\n",
    "             'No. of Applications Rejected',\n",
    "             'No. of Applications Pullback',\n",
    "             'No. of Applications Closed',\n",
    "             'No. of Constructed Toilet Photo Uploaded',\n",
    "             'No. of Constructed Toilet Photo Approved',\n",
    "             'No. of Commenced Toilet Photo ',\n",
    "             'No. of Constructed Toilet Photo through Swachhalaya',\n",
    "             ]\n",
    "a=pd.DataFrame()\n",
    "for j in range(2,len(state_tags)-1):\n",
    "    WebDriverWait(browser,20).until(EC.presence_of_all_elements_located((By.ID,'ContentPlaceHolder1_gvApplicationListState')))\n",
    "    state_click=browser.find_element_by_id(state_tags[j]['id'])\n",
    "    state_click.click()\n",
    "    state=state_tags[j].text\n",
    "    element= WebDriverWait(browser, 20).until(EC.presence_of_all_elements_located((By.ID,'ContentPlaceHolder1_gvApplicationListDistrict')))\n",
    "    soup2=BeautifulSoup(browser.page_source)\n",
    "    soup=soup2.find('table',id='ContentPlaceHolder1_gvApplicationListDistrict')\n",
    "    element2= WebDriverWait(browser, 20).until(EC.presence_of_all_elements_located((By.ID,'ContentPlaceHolder1_gvApplicationListDistrict')))\n",
    "    districts=soup.findAll('a')\n",
    "\n",
    "    for i in range(0,len(districts)):\n",
    "        WebDriverWait(browser, 20).until(EC.presence_of_all_elements_located((By.ID,'ContentPlaceHolder1_gvApplicationListDistrict')))\n",
    "        element=browser.find_element_by_id(districts[i]['id'])\n",
    "        element.click()\n",
    "        district=districts[i].text\n",
    "        element= WebDriverWait(browser, 20).until(EC.presence_of_all_elements_located((By.ID,'ContentPlaceHolder1_gvApplicationListULB')))\n",
    "        soup3=BeautifulSoup(browser.page_source)\n",
    "        soup4=soup3.find('table',id='ContentPlaceHolder1_gvApplicationListULB')\n",
    "        ulb=soup4.findAll('a')\n",
    "        \n",
    "        \n",
    "        for k in range(0,len(ulb)):\n",
    "            WebDriverWait(browser, 20).until(EC.presence_of_all_elements_located((By.ID,'ContentPlaceHolder1_gvApplicationListULB')))\n",
    "            element=browser.find_element_by_id(ulb[k]['id'])\n",
    "            element.click()\n",
    "            ulb_name=ulb[k].text\n",
    "            #for ulb having no records\n",
    "            if len(ulb[k])<1:\n",
    "                b=[0]*14\n",
    "                c=pd.DataFrame(b)\n",
    "                c=c.transpose()\n",
    "                final_table_ward=c\n",
    "                final_table_ward.columns=headers\n",
    "                final_table_ward.insert(1,'State',state)\n",
    "                final_table_ward.insert(2,'District',district)\n",
    "                final_table_ward.insert(3,'ULB',ulb_name)\n",
    "                a=pd.concat([a,final_table_ward])\n",
    "            element=WebDriverWait(browser, 20).until(EC.presence_of_all_elements_located((By.ID,'ContentPlaceHolder1_gvApplicationListWARD')))\n",
    "            soup5=BeautifulSoup(browser.page_source)\n",
    "            soup6=soup5.find('table',id='ContentPlaceHolder1_gvApplicationListWARD')\n",
    "            ward=soup6.findAll('tr')\n",
    "            # for ward having no records\n",
    "            if(len(ward))<4:\n",
    "                b=[0]*14\n",
    "                c=pd.DataFrame(b)\n",
    "                c=c.transpose()\n",
    "                final_table_ward=c \n",
    "            full_data_ward=[]\n",
    "            for l in range(4,len(ward)): \n",
    "                full_data_ward.append(ward[l].text.split('\\n'))\n",
    "            clean_data_ward=[]\n",
    "            for m in range(len(full_data_ward)):\n",
    "                space_to_empty=[x.strip() for x in full_data_ward[m]]\n",
    "                space_clean_list=[x for x in space_to_empty if x]\n",
    "                clean_data_ward.append(space_clean_list)\n",
    "                final_table_ward=pd.DataFrame(clean_data_ward)\n",
    "                final_table_ward=final_table_ward.drop_duplicates()\n",
    "                #column 13 just have symbol, so I am going to drop it\n",
    "                del final_table_ward[13]\n",
    "            final_table_ward.columns=headers\n",
    "            final_table_ward.insert(1,'State',state)\n",
    "            final_table_ward.insert(2,'District',district)\n",
    "            final_table_ward.insert(3,'ULB',ulb_name)\n",
    "            a=pd.concat([a,final_table_ward])\n",
    "            browser.find_element_by_link_text(districts[i].text.strip()).click()\n",
    "        browser.find_element_by_link_text(state_tags[j].text.strip()).click()\n",
    "    browser.get('http://swachhbharaturban.gov.in/ihhl/RPTApplicationSummary.aspx')\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ward_wise_data=a\n",
    "ward_wise_data=ward_wise_data.dropna()\n",
    "ward_wise_data.to_csv('ward_wise_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statewise,Districtwise,ulbwise data can also be prepared by using group by on ward_wise data.Here is code for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "ulb_wise_data=ward_wise_data.groupby(['State','District','ULB']).sum().reset_index()\n",
    "ulb_wise_data.drop(['SNO','Ward No.'],axis=1,inplace=True)\n",
    "ulb_wise_data.insert(0,'SNO',1)\n",
    "ulb_wise_data['SNO']=np.arange(1,len(ulb_wise_data['SNO'])+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "district_wise_data=ulb_wise_data.groupby(['State','District']).sum().reset_index()\n",
    "district_wise_data.drop(['SNO'],axis=1,inplace=True)\n",
    "district_wise_data.insert(0,'SNO',1)\n",
    "district_wise_data['SNO']=np.arange(1,len(district_wise_data['SNO'])+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_wise_data=district_wise_data.groupby(['State']).sum().reset_index()\n",
    "state_wise_data.drop(['SNO'],axis=1,inplace=True)\n",
    "state_wise_data.insert(0,'SNO',1)\n",
    "state_wise_data['SNO']=np.arange(1,len(state_wise_data['SNO'])+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
